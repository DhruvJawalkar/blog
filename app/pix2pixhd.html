<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>pix2pixHD</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.css" rel="stylesheet">
  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
      <div class="container">
        <a style="font-size: 1rem" class="navbar-brand" href="index.html">pix2pixHD: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="about.html">About</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-color: grey">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-12 col-md-12 mx-auto">
            <div style="padding-top: 25px;" class="demo-section">
            <h2 class="subheading demo-heading float-left"></h2>
            <div style="height: 600px; padding-top: 0; display: table-cell; vertical-align: middle;" class="result-div" id="result">
                <img style="max-height: 350px; display: inline-block;" src="images/pix2pixhd/preview-sem-label.png"/>
                <img style="max-height: 350px; display: inline-block;" src="images/pix2pixhd/preview-op.jpg"/>
            </div>
            </div>
            <div class="clear"></div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container main-content">
        <div class="row">
          <div class="col-lg-12 col-md-12 mx-auto"> 
              <p>
                <h2>Novelty Overview</h2>
                Paper aims to generate 2048 x 1024 realistic looking images given semantic label maps and also develops novel method to allow controllable output generation from GANs.
                Given the inference time of the network is ~30ms on a GTX 1080ti GPU, allows for a realtime interactive editing framework for users to change the scene as desired.   
              </p>
              <p style="margin-top: 100px;">
                <h2>Challenges</h2>
                <ul>
                    <li>Training GANs at higher resolutions was found to be unstable by previous methods</li>
                    <li>GAN output results were not realistic enough</li>
                    <li>No intuitive control over outputs generated by GANs</li>
                </ul>
              </p>
              <p style="margin-top: 100px;">
                <h2>Method</h2>    
                <ul>
                    <li><b>Multi-scale Generator</b> - To stabilize training, builds from coarse (1024x512) to fine (2048x1024) resolution</li>
                    <li><b>Multi-scale Discriminator</b> - To ensure scene consistency at all levels (op/4, op/2, op) improves visual result</li>
                    <li><b>Improved Adversarial Loss</b> - For stable training and better visual results</li>
                    <li><b>Object Instance Maps</b> - For better boundary separation between object instances</li>
                    <li><b>Feature Encoder Network</b> - To learn representation of an object instance as a feature vector, to allow interactive editing</li>
                </ul>
              </p>
              <div class="clear"></div>
              <p style="margin-top: 100px;">
                  <h2>Multi-scale Generator</h2>    
                  <ul>
                      <li>First trains an Autoencoder style network G1, at smaller resolution (1024x512)</li>
                      <li>Then appends G2, to generate at higher resolution (2048x1024), this approach leads to stable training</li>
                      <li>G2 takes coarse output from G1 and refines it to produce final result</li>
                  </ul>
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:80%; display:inline-block;" src="images/pix2pixhd/generator-net.png"/>
              </div>
              <div class="clear"></div>
              <span class="caption text-muted">The Generator Network, G1, G2</span>
              <p style="margin-top: 100px;">
                  <h2>Multi-scale Discriminator</h2>    
                  <ul>
                      <li>3 Discriminators have been used to operate on op, op/2, op/4 resolution, to ensure resemblance at all levels</li>
                      <li>Trained from course to fine to ensure stablility</li>
                      <li><b>70x70 Patch GAN Architecture</b> - Evaluates image in 70x70 Patches, resulting in smaller kernels and efficient memory usage, averages all patch results for final decision</li>
                      <li>If Ck -> 4Ã—4Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. <br/>Architecture : <b>C64-C128-C256-C512</b></li>
                  </ul>
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:100%; display:inline-block;" src="images/pix2pixhd/patchsize-variation.png"/>
              </div>
              <div class="clear"></div>
              <span class="caption text-muted">Study done in the pix2pix paper using the Patch GAN approach, smaller patch size creates artifacts in output, 70x70 yeilded better results w/o many artifacts ~ similar when compared to then full resolution of 286x286 </span>
              <p style="margin-top: 100px;">
                  <h2>Improved Adversarial Loss</h2>    
                  <ul>
                      <li>Rather than having traditional GAN objective function as loss, add featue matching terms to improve visual similarity between Generator op vs GT</li>
                      <li>Adds Discriminator feature matching loss, difference in activations at every level</li>
                      <li>Additional feature matching loss on activations from pre-trained VGG network</li>
                  </ul>
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:60%; display:inline-block;" src="images/pix2pixhd/overall-loss.png"/>
                  <div>
                      <img style="width:40%; display:inline-block;" src="images/pix2pixhd/dfm-loss.png"/>
                  </div>
              </div>
              <div class="clear"></div>
              <span class="caption text-muted">Improved Adversarial Loss</span>
              <p style="margin-top: 100px;">
                <b>Traditional GAN objective</b>    
                <ul>
                    <li>Generator Network to generate desired output from noise or certain input</li>
                    <li>Discriminator Network acts like a trainable loss function, to 'learn' on its own to distinguish</li>
                    <li>Standard use of hand written L1 or L2 loss would result in blurry results</li>
                    <li>Overall training goes like, train Discriminator first to better discriminate Generator op(fake ones) vs GT(Actual op)</li>
                    <li>Then train Generator with this large gap in distinguishion as the loss</li>
                    <li>Repeat above 2 steps till Generator outputs are indistinguishable, it has successfully modeled the real distribution</li>
                </ul>
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:50%; display:inline-block;" src="images/pix2pixhd/init-gan-objective.png"/>
                  <img style="width:40%; display:inline-block;" src="images/pix2pixhd/logx.png"/>
              </div>
              <div class="clear"></div>
              <span class="caption text-muted">Traditional GAN objective function</span>
              <p style="margin-top: 100px;">
                <b>Using Instance Maps</b>    
                <ul>
                    <li>Provides better sepration of object instances in final output</li>
                </ul>
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:50%; display:inline-block;" src="images/pix2pixhd/boundary.png"/>
                  <img style="width:40%; display:inline-block;" src="images/pix2pixhd/boundary-res.png"/>
              </div>
              <div class="clear"></div>
              <span class="caption text-muted">Improvement from using Instance Maps</span>


              <p style="margin-top: 100px;">
                  <h2>Feature Encoder Network</h2>
                  <ul>
                    <li>Aim is to learn a feature vector at the object instance level</li>
                    <li>With added input of 3d feature vector, averaged per instance, tries to learn input instance vector vs visual op</li>
                    <li>After joint training, they perform k-means clutering with 10 final clusters over entire training set per semantic category</li>
                    <li>Cluster centers are provided as visual options in the final editable framework</li>
                  </ul>    
              </p>
              <div class="clear"></div>
              <div style="margin: 30px 0 0 0;" class="text-center">
                  <img style="width:50%; display:inline-block;" src="images/pix2pixhd/city_short.gif"/>
                  <img style="width:35%; display:inline-block;" src="images/pix2pixhd/feature-encoder-net.png"/>
              </div>
              <div class="clear"></div>
              <p style="margin-top: 150px;">
                <h2>Project Demo</h2>
                <div style="margin-top:30px;" class="text-center">
                  <iframe width="1024" height="575" src="https://www.youtube.com/embed/3AIpPlzM_qs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <div class="clear"></div>
            </p>
              <p style="margin-top:100px;">  
                <h2 class="section-heading">Links:</h2> 
                <ul>
                  <li><a class="underline" href="https://arxiv.org/pdf/1711.11585.pdf">Pix2PixHD</a></li>
                  <li><a class="underline" href="https://tcwang0509.github.io/pix2pixHD/">Project Website</a></li>
                  <li><a class="underline" href="https://github.com/NVIDIA/pix2pixHD">Github Repo</a></li>
                </ul>
                <div class="clear"></div>
              </p>
          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://github.com/DhruvJawalkar/">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/dhruv-jawalkar-43b8816b/">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Your Website 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <!-- Custom scripts for this template -->
  </body>

</html>
